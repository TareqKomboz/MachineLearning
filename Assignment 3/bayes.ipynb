{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globe Tossing - A Bayesian Learning Exercise\n",
    "\n",
    "## Introduction\n",
    "Suppose you have a globe representing our planet, the Earth. This version of the world is small enough to hold in your hands. You are curious how much of the surface is covered in water. You adopt the following strategy: You will toss the globe up in the air. When you catch it, you will record whether or not the surface under your right index finger is water or land. Then you toss the globe up in the air again and repeat the procedure. This strategy generates a sequence of samples from the globe. The first nine samples could look like the following sequence: $\\mathcal{D} = (W L W W W L W L W)$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Objective:\n",
    "In this task, you will implement a Bayesian updating function to estimate the proportion of water on Earth based on sequence of tosses you will generate. You will also create a plot to visualize how the Bayesian model learns and refines its estimate as more data is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "\n",
    "1. Initialize your prior belief about the proportion $p$ of water on Earth. Represent this belief as a uniform distribution on the interval [0, 1]. You can use the `numpy` library to create an array of 1000 evenly spaced values between 0 and 1, representing potential proportions of water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the prior belief\n",
    "\n",
    "proportions =  # sol\n",
    "prior =  # sol\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a function called `likelihood` that takes two arguments: the proportion of water and an observed outcome (either 'W' or 'L'). The function should return the likelihood of observing the outcome given the proportion of water. For example, if the proportion of water is 0.6 and the outcome is 'W', the likelihood is 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the likelihood function\n",
    "\n",
    "def likelihood(proportion, outcome):\n",
    "    # sol\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a function called `bayesian_update` that takes three arguments: the prior distribution, the proportion of water array, and the observed outcome. The function should perform the Bayesian update by calculating the likelihood of the observed outcome for each proportion in the array, and then multiplying it by the corresponding prior value. Normalize the resulting distribution so that it sums to 1. This normalized distribution is your new posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create the bayesian_update function\n",
    "\n",
    "def bayesian_update(prior, proportions, outcome):\n",
    "    # sol\n",
    "    return posterior\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Simulate a series of globe tosses with a known true proportion of water. You can use the `numpy random.choice` function to generate random outcomes ('W' or 'L') based on the true proportion. For this exercise, let's assume the true proportion of water is 0.71. Generate 9 toss outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Simulate globe tosses\n",
    "\n",
    "true_proportion = 0.71\n",
    "toss_outcomes =  # sol\n",
    "print(toss_outcomes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Update your prior belief iteratively using the `bayesian_update` function and the observed outcomes. Perform the updates after every toss and store the resulting posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Update prior belief iteratively and store posterior distributions\n",
    "\n",
    "posteriors = []\n",
    "toss_sequences = []\n",
    "# sol\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create a plot to visualize how the Bayesian model learns and refines its estimate as more data is collected. On the x-axis, show the proportion of water, and on the y-axis, show the plausibility (probability density) of each proportion. Plot the prior distribution and the posterior distributions after each toss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Plot the learning process in a grid of plots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# sol\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - A Naive Bayes Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Understanding\n",
    "Consider a document classification problem, where you are given a dataset of text documents with their corresponding categories. You are asked to classify new documents into one of the given categories using the Naive Bayes classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the Naive Bayes assumption in the context of document classification. Why is this assumption considered \"naive\"?\n",
    "\n",
    "1 - Example solution: In the context of document classification, the Naive Bayes assumption is that the features (words in this case) are conditionally independent given the class. In other words, the presence of one word does not influence the presence of another word in a document, given the document's category. This assumption is considered \"naive\" because, in reality, words in a document often have dependencies and are not completely independent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write the formula for the Naive Bayes classifier in terms of the likelihood, prior, and posterior probabilities, and explain each term's meaning.\n",
    "\n",
    "2 - Example Solution: The formula for the Naive Bayes classifier is: $\n",
    "P(C_k|X) = \\frac{P(X|C_k) \\cdot P(C_k)}{P(X)}\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "- $P(C_k|X)$ is the posterior probability of class $C_k$ given the features (words) $X$\n",
    "- $P(X|C_k)$ is the likelihood of features (words) $X$ given class $C_k$\n",
    "- $P(C_k)$ is the prior probability of class $C_k$\n",
    "- $P(X)$ is the probability of features (words) $X$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Discuss how the Naive Bayes classifier handles cases where some words in a new document are not present in the training data. Explain the concept of \"smoothing\" and how it helps in this situation.\n",
    "\n",
    "3 - Example Solution: When some words in a new document are not present in the training data, the likelihood of those words given a class becomes zero, causing the entire likelihood for that class to become zero. This can lead to incorrect classifications. Smoothing is a technique to handle this situation. One common method is Laplace smoothing, which adds a small constant α (usually 1) to each word's frequency count. This ensures that no word has a zero likelihood and allows the Naive Bayes classifier to make reasonable predictions even when encountering new words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Implement a Naive Bayes classifier in Python to classify documents into one of two categories: \"positive\" or \"negative.\" You are given a training dataset of movie reviews and their corresponding labels (either \"positive\" or \"negative\").\n",
    "\n",
    "Your dataset consists of (input,output) pairs with the following shape:\n",
    "\n",
    "(document, label) = (movie_review, positive/negative)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "Write a function `preprocess(document)` that takes a single document as input and returns a list of words after tokenization, lowercase conversion, and removal of punctuation and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the document\n",
    "def preprocess(document):\n",
    "\n",
    "\n",
    "stop_words = {\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \"has\", \"he\",\n",
    "              \"in\", \"is\", \"it\", \"its\", \"of\", \"on\", \"that\", \"the\", \"to\", \"was\", \"were\", \"will\", \"with\"}\n",
    "symbols = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "\n",
    "def preprocess(document):\n",
    "    # sol\n",
    "    return words\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Naive Bayes classifier\n",
    "Write a function `train(training_data)` that takes the training dataset as input and returns a dictionary containing the prior probabilities for each class and the likelihoods for each word given each class. Apply Laplace smoothing with $α=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Naive Bayes model\n",
    "\n",
    "def train(training_data):\n",
    "    model = {}\n",
    "    # sol\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `classify(document, model)` that takes a document and the model (dictionary) returned by the `train()` function as input and returns the predicted class (\"positive\" or \"negative\") for the given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a document using the Naive Bayes model\n",
    "\n",
    "def classify(document, model):\n",
    "    # sol\n",
    "    return predicted_class\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation using the small dataset of movie reviews provided below, and provide the classification results for a few example reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Naive Bayes model\n",
    "training_data = [\n",
    "    (\"I absolutely loved this movie! The acting was superb.\", \"positive\"),\n",
    "    (\"The movie had a great plot and fantastic performances.\", \"positive\"),\n",
    "    (\"This is one of the best movies I have ever seen.\", \"positive\"),\n",
    "    (\"The cinematography was amazing, and the story was captivating.\", \"positive\"),\n",
    "    (\"This movie is an all-time favorite of mine.\", \"positive\"),\n",
    "    (\"The acting and direction were top-notch.\", \"positive\"),\n",
    "    (\"This film was a wonderful blend of humor and drama.\", \"positive\"),\n",
    "    (\"The story was original and the characters were well-developed.\", \"positive\"),\n",
    "    (\"A must-watch movie for everyone!\", \"positive\"),\n",
    "    (\"The visual effects were stunning, and the soundtrack was perfect.\", \"positive\"),\n",
    "    (\"Incredible performances by the entire cast.\", \"positive\"),\n",
    "    (\"A heartwarming and inspiring story.\", \"positive\"),\n",
    "    (\"An unforgettable cinematic experience.\", \"positive\"),\n",
    "    (\"The movie had me on the edge of my seat!\", \"positive\"),\n",
    "    (\"A thrilling and emotional rollercoaster.\", \"positive\"),\n",
    "    (\"I can't stop thinking about how good this movie was.\", \"positive\"),\n",
    "    (\"A timeless classic that I'll watch again and again.\", \"positive\"),\n",
    "    (\"The movie was charming and full of surprises.\", \"positive\"),\n",
    "    (\"The plot twists kept me guessing until the very end.\", \"positive\"),\n",
    "    (\"An excellent adaptation of the book.\", \"positive\"),\n",
    "    (\"I found this movie to be boring and uninspired.\", \"negative\"),\n",
    "    (\"The plot was predictable, and the acting was mediocre.\", \"negative\"),\n",
    "    (\"This movie was a complete waste of time.\", \"negative\"),\n",
    "    (\"The characters were unlikable, and the story was dull.\", \"negative\"),\n",
    "    (\"I can't believe I wasted two hours of my life watching this.\", \"negative\"),\n",
    "    (\"The special effects were laughable, and the acting was wooden.\", \"negative\"),\n",
    "    (\"I have never been so disappointed by a movie in my life.\", \"negative\"),\n",
    "    (\"The dialogue was cheesy, and the storyline was confusing.\", \"negative\"),\n",
    "    (\"The movie had no redeeming qualities whatsoever.\", \"negative\"),\n",
    "    (\"The acting was terrible, and the plot made no sense.\", \"negative\"),\n",
    "    (\"This film was a mess from start to finish.\", \"negative\"),\n",
    "    (\"I would not recommend this movie to anyone.\", \"negative\"),\n",
    "    (\"I walked out of the theater halfway through the movie.\", \"negative\"),\n",
    "    (\"A lackluster performance by the lead actor.\", \"negative\"),\n",
    "    (\"I couldn't wait for this movie to end.\", \"negative\"),\n",
    "    (\"The movie tried too hard to be funny, but it just wasn't.\", \"negative\"),\n",
    "    (\"I was so disappointed with the ending.\", \"negative\"),\n",
    "    (\"The movie was extremely slow-paced and uneventful.\", \"negative\"),\n",
    "    (\"Poorly written and poorly executed.\", \"negative\"),\n",
    "    (\"The movie was forgettable and not worth watching.\", \"negative\"),\n",
    "]\n",
    "\n",
    "model = train(training_data)\n",
    "\n",
    "test_reviews = [\n",
    "    (\"This movie was a masterpiece with a fantastic plot and great acting!\", \"positive\"),\n",
    "    (\"I didn't enjoy this movie at all. It was boring and predictable.\", \"negative\"),\n",
    "]\n",
    "\n",
    "# sol\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the performance of your model, any potential pitfalls or things you could implement to improve it? Keep your answer short. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SOL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
