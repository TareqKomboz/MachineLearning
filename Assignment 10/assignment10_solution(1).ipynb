{"cells":[{"cell_type":"markdown","metadata":{"id":"bUClzaVnDcjx"},"source":["# Assignment 10: Neural Networks\n","\n","Only use the already imported library `numpy` and the Python standard library. For the evaluation you may also use scikit-learn (`sklearn`) and `matplotlib`. Make sure that the dataset `airfoil_self_noise.csv` is in the same directory as the notebook.\n","\n","List your team members (name and immatriculation number) and indicate whether you are a B.Sc. Data Science or other group in the following cell:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"CjQJRgiVDcj0","executionInfo":{"status":"ok","timestamp":1689552012066,"user_tz":-120,"elapsed":1054,"user":{"displayName":"mojtaba nayyeri","userId":"16541089370646536836"}}},"outputs":[],"source":["from typing import List\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def load_dataset(path):\n","    from sklearn.model_selection import train_test_split\n","\n","    data = np.genfromtxt(path)\n","    X, y = data[:, :5], data[:, 5]\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)\n","\n","    return X_train, X_test, y_train, y_test\n","\n","\n","X_train, X_test, y_train, y_test = load_dataset('airfoil_self_noise.csv')"]},{"cell_type":"markdown","metadata":{"id":"K6eAtVIvDcj1"},"source":["- *Your names here*"]},{"cell_type":"markdown","metadata":{"id":"-ir_lX0eDcj2"},"source":["## Task 3: Implementing a Feedforward Neural Network\n","\n","In this task, you will implement a feedforward neural network for regression. The hyperparameters of the model are:\n","- `input_dim`: The dimension of the input vector.\n","- `output_dim`: The dimension of the output vector.\n","- `width`: The dimension of each hidden layer.\n","- `depth`: The number of hidden layers. For B.Sc. Data Science students, this parameter is constant with a value of 1.\n","- `learning_rate`: The learning rate for gradient descent.\n","- `epochs`: The number of epochs/iterations performed during training.\n","\n","B.Sc. Data Science only have to implement for a single hidden layer, i.e. `depth = 1`. All other students have to implement the network for any `depth >= 1`.\n","\n","The activation function for each hidden layer is ReLU ($g(x) = max(0, x)$). The output layer uses the identity as activation, since our objective is regression.\n","\n","You have to implement the `FeedforwardNeuralNetworkRegressor`.\n","\n","The `__init__` method initializes the network.\n","Initialize each weight and bias randomly with a standard Gaussian distribution using the numpy function `numpy.random.normal` with default parameters.\n","\n","The `fit` method trains the network.\n","Use backpropagation with gradient descent similar to Task 2.\n","Use the whole training data set for each training epoch.\n","Use the mean squared error as loss function.\n","\n","The `predict` method computes the forward-pass of the network.\n","\n","We have provided code for preprocessing and validating your model."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"E5Xd1L7yDcj2","executionInfo":{"status":"ok","timestamp":1689552012069,"user_tz":-120,"elapsed":11,"user":{"displayName":"mojtaba nayyeri","userId":"16541089370646536836"}}},"outputs":[],"source":["class FeedforwardNeuralNetworkRegressor(object):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        output_dim: int,\n","        hidden_dims: List[int],\n","        depth: int,\n","        learning_rate: float,\n","        epochs: int\n","    ):\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.hidden_dims = hidden_dims\n","        self.depth = depth\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        # Create list for all weights and biases\n","        # Create a randomly initizalized weight and bias matrix for the input layer\n","        self.weights = [np.random.normal(size=(input_dim, hidden_dims[0]))]\n","        self.biases = [np.random.normal(size=(hidden_dims[0]))]\n","        # Create a randomly initizalized weight and bias matrix for each hidden layer\n","        for i in range(self.depth-1):\n","            self.weights.append(np.random.normal(size=(hidden_dims[i], hidden_dims[i+1])))\n","            self.biases.append(np.random.normal(size=(hidden_dims[i+1])))\n","        # Create a randomly initizalized weight and bias matrix for the output layer\n","        self.weights.append(np.random.normal(size=(hidden_dims[depth-1], output_dim)))\n","        self.biases.append(np.random.normal(size=(output_dim)))\n","\n","    def predict(self, X):\n","        # Run the forward pass and return the predicted output\n","        _, _, pred = self.forward(X)\n","        return pred\n","\n","\n","    def relu(self, layer):\n","        return np.maximum(0, layer)\n","\n","    def relu_der(self, layer):\n","        return 1*(layer > 0)\n","\n","    def mse_loss(self, y_pred, y_true):\n","        return np.mean((y_pred - y_true)*(y_pred - y_true))\n","\n","    def mse_loss_der(self, y_pred, y_true):\n","        return 2 * (y_pred - y_true)\n","\n","    def forward(self, X):\n","        # Implementation of the forward pass\n","        # Store the intermediate results of the hidden layers for the back propagation\n","        hidden_x = [X]\n","        hidden_z = []\n","        for i in range(self.depth):\n","            hidden_z.append(hidden_x[i] @ self.weights[i] + self.biases[i])\n","            hidden_x.append(self.relu(hidden_z[i]))\n","        output = hidden_x[self.depth] @ self.weights[self.depth] + self.biases[self.depth]\n","\n","        return hidden_z, hidden_x, output\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"UXlnn5XpDcj4","executionInfo":{"status":"ok","timestamp":1689552013059,"user_tz":-120,"elapsed":12,"user":{"displayName":"mojtaba nayyeri","userId":"16541089370646536836"}}},"outputs":[],"source":["y_train = y_train.reshape((-1, 1))\n","y_test = y_test.reshape((-1, 1))\n","\n","def normalizeData(data, minData, maxData):\n","    return (data - minData)/(maxData - minData)\n","\n","Xmin = X_train.min(axis=0)\n","Xmax = X_train.max(axis=0)\n","Ymin = y_train.min(axis=0)\n","Ymax = y_train.max(axis=0)\n","\n","X_train = normalizeData(X_train, Xmin, Xmax)\n","X_test = normalizeData(X_test, Xmin, Xmax)\n","y_train = normalizeData(y_train, Ymin, Ymax)\n","y_test = normalizeData(y_test, Ymin, Ymax)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AzWI0ThTDcj4","outputId":"06d9c973-4a14-4432-c8bc-34e0b532d9c9","executionInfo":{"status":"ok","timestamp":1689552013060,"user_tz":-120,"elapsed":8,"user":{"displayName":"mojtaba nayyeri","userId":"16541089370646536836"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Test error: 0.5195594697573351\n"]}],"source":["def mse_loss(y_pred, y_true):\n","    return np.mean((y_pred - y_true)*(y_pred - y_true))\n","\n","\n","np.random.seed(2023)\n","\n","input_dim = X_train.shape[1]\n","output_dim = y_train.shape[1]\n","nn = FeedforwardNeuralNetworkRegressor(input_dim, output_dim, [5], 1, 0.1, 100)\n","# train_error = nn.fit(X_train, y_train)\n","# print(\"Final training error ( Epoch\", len(train_error), \"):\", train_error[len(train_error)-1])\n","\n","y_pred = nn.predict(X_test)\n","test_error = mse_loss(y_pred, y_test)\n","print(\"Test error:\", test_error)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LsFCp4CIDcj5"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}