{"cells":[{"cell_type":"markdown","metadata":{"id":"bUClzaVnDcjx"},"source":["# Assignment 11-part1: Neural Networks and PyTorch\n","\n","This assignment is the continuation of the last week assignment and includes implementation of packpropagation from scratch, and also implementation of feedforward neural networks in PyTorch.\n","\n"]},{"cell_type":"markdown","source":["Only use the already imported library `numpy` and the Python standard library. For the evaluation you may also use scikit-learn (`sklearn`) and `matplotlib`. Make sure that the dataset `airfoil_self_noise.csv` is in the same directory as the notebook.\n","\n","List your team members (name and immatriculation number) and indicate whether you are a B.Sc. Data Science or other group in the following cell:"],"metadata":{"id":"bAaGhsjyuENj"}},{"cell_type":"markdown","source":["- *Your names here*"],"metadata":{"id":"W830uyJQYv5C"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjQJRgiVDcj0"},"outputs":[],"source":["from typing import List\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def load_dataset(path):\n","    from sklearn.model_selection import train_test_split\n","\n","    data = np.genfromtxt(path)\n","    X, y = data[:, :5], data[:, 5]\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)\n","\n","    return X_train, X_test, y_train, y_test\n","\n","\n","X_train, X_test, y_train, y_test = load_dataset('airfoil_self_noise.csv')"]},{"cell_type":"markdown","metadata":{"id":"-ir_lX0eDcj2"},"source":["## Task 3: Implementing a Feedforward Neural Network\n","\n","In this task, you will implement a feedforward neural network for regression. The hyperparameters of the model are:\n","- `input_dim`: The dimension of the input vector.\n","- `output_dim`: The dimension of the output vector.\n","- `width`: The dimension of each hidden layer.\n","- `depth`: The number of hidden layers. For B.Sc. Data Science students, this parameter is constant with a value of 1.\n","- `learning_rate`: The learning rate for gradient descent.\n","- `epochs`: The number of epochs/iterations performed during training.\n","\n","B.Sc. Data Science only have to implement for a single hidden layer, i.e. `depth = 1`. All other students have to implement the network for any `depth >= 1`.\n","\n","The activation function for each hidden layer is ReLU ($g(x) = max(0, x)$). The output layer uses the identity as activation, since our objective is regression.\n","\n","You have to implement the `FeedforwardNeuralNetworkRegressor`.\n","\n","The `__init__` method initializes the network.\n","Initialize each weight and bias randomly with a standard Gaussian distribution using the numpy function `numpy.random.normal` with default parameters.\n","\n","The `fit` method trains the network.\n","Use backpropagation with gradient descent similar to Task 2.\n","Use the whole training data set for each training epoch.\n","Use the mean squared error as loss function.\n","\n","The `predict` method computes the forward-pass of the network.\n","\n","We have provided code for preprocessing and validating your model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5Xd1L7yDcj2"},"outputs":[],"source":["class FeedforwardNeuralNetworkRegressor(object):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        output_dim: int,\n","        hidden_dims: List[int],\n","        depth: int,\n","        learning_rate: float,\n","        epochs: int\n","    ):\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.hidden_dims = hidden_dims\n","        self.depth = depth\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        # Create list for all weights and biases\n","        # Create a randomly initizalized weight and bias matrix for the input layer\n","\n","\n","        # Create a randomly initizalized weight and bias matrix for each hidden layer\n","\n","\n","\n","        # Create a randomly initizalized weight and bias matrix for the output layer\n","\n","\n","\n","\n","    def fit(self, X, y):\n","        errors = np.zeros(self.epochs)\n","        current_ep = self.epochs-1\n","        # Add training process\n","        for ep in range(self.epochs):\n","\n","\n","\n","        return errors[:current_ep+1]\n","\n","    def predict(self, X):\n","        # Run the forward pass and return the predicted output\n","        _, _, pred = self.forward(X)\n","        return pred\n","\n","\n","    def relu(self, layer):\n","        return np.maximum(0, layer)\n","\n","    def relu_der(self, layer):\n","        return 1*(layer > 0)\n","\n","    def mse_loss(self, y_pred, y_true):\n","        return np.mean((y_pred - y_true)*(y_pred - y_true))\n","\n","    def mse_loss_der(self, y_pred, y_true):\n","        return 2 * (y_pred - y_true)\n","\n","    def forward(self, X):\n","        # Implementation of the forward pass\n","        # Store the intermediate results of the hidden layers for the back propagation\n","        hidden_x = [X]\n","        hidden_z = []\n","\n","\n","        return hidden_z, hidden_x, output\n","\n","    def backward(self, pred, y):\n","        # delta = intermediate loss derivatives\n","\n","\n","        return biases_der, weights_der\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXlnn5XpDcj4"},"outputs":[],"source":["y_train = y_train.reshape((-1, 1))\n","y_test = y_test.reshape((-1, 1))\n","\n","def normalizeData(data, minData, maxData):\n","    return (data - minData)/(maxData - minData)\n","\n","Xmin = X_train.min(axis=0)\n","Xmax = X_train.max(axis=0)\n","Ymin = y_train.min(axis=0)\n","Ymax = y_train.max(axis=0)\n","\n","X_train = normalizeData(X_train, Xmin, Xmax)\n","X_test = normalizeData(X_test, Xmin, Xmax)\n","y_train = normalizeData(y_train, Ymin, Ymax)\n","y_test = normalizeData(y_test, Ymin, Ymax)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AzWI0ThTDcj4"},"outputs":[],"source":["def mse_loss(y_pred, y_true):\n","    return np.mean((y_pred - y_true)*(y_pred - y_true))\n","\n","\n","np.random.seed(2023)\n","\n","input_dim = X_train.shape[1]\n","output_dim = y_train.shape[1]\n","nn = FeedforwardNeuralNetworkRegressor(input_dim, output_dim, [5], 1, 0.1, 100)\n","train_error = nn.fit(X_train, y_train)\n","print(\"Final training error ( Epoch\", len(train_error), \"):\", train_error[len(train_error)-1])\n","\n","y_pred = nn.predict(X_test)\n","test_error = mse_loss(y_pred, y_test)\n","print(\"Test error:\", test_error)\n","\n","plt.figure()\n","plt.plot(train_error)\n","plt.show()"]},{"cell_type":"markdown","source":["## PyTorch:\n","In this part of the assignment, you will implement deep learning at the example of classification and regression tasks with PyTorch.\n","You are free to use any packages from the standard library and third-party libraries. However, to solve the assignment you will only need numpy, matplotlib, torch, and torchvision.\n","\n","The following are covered in the rest of this assignment:\n","\n","\n","1.   Installation\n","2.   Tensor\n","3.   Autograd\n","4.   Building a model\n","5.   Training models\n"],"metadata":{"id":"JTOJ6XB7mBSx"}},{"cell_type":"markdown","source":["## Installation\n","\n","Go to [PyTorch/Get Started](https://pytorch.org/get-started/locally/) to find installation instructions for torch and torchvisio for your specific hardware, operating system, and Python environment. If you are unsure about your hardware, select CPU as Compute Platform.\n","\n","For example, installing for Windows via pip on CPU requires the following command:\n","```shell\n","pip3 install torch torchvision torchaudio\n","```\n","\n","You might need to adjust some code in the following tasks, if you want to run it on a GPU.\n","\n","\n","\n","At the first step, torch and its neural networks module, i.e., torch.nn should be imported."],"metadata":{"id":"h8JOkKqIiEkJ"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils import data\n","import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"ldeeX-cMrBQi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tensors\n","\n","Torch operates on `torch.Tensors`. These tensors are very similar to Numpy's n-dimensional arrays. Most operations you know from numpy, such as slicing, arithmetics, etc. work similarly with tensors. Note that you have to use the torch equivalents of methods such as sum and mean, since Torch can only support autodifferentiation with such methods. Arithmetic operations between tensors  and slicing automatically support autodifferentiation.\n","\n","Here there are a few basic tensor manipulations."],"metadata":{"id":"3pBICMdFrL7b"}},{"cell_type":"code","source":["x = torch.tensor([[1, 4], [2,3]])\n","print('x = ', x)\n","print('type =' , x.dtype)\n","\n","x = x.to(torch.float32)\n","\n","print('\\n x = ', x)\n","print('type =' , x.dtype)\n","\n","y = torch.tensor([[1.0, 4.0], [2.0, 3.0]])\n","\n","print('\\n y = ', y)\n","print('type =' , y.dtype)\n","\n","# Tensor with random values\n","\n","z = torch.rand(2, 2)\n","print('\\n A random tensor:')\n","print(z)\n","\n","# Basic arithmatic operations\n","k1 = y + x\n","k2 = y * x\n","k3 = y @ x\n","\n","print('\\n Addition and Multiplication:')\n","print(k1, '\\n', k2, '\\n', k3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tunOpekSreK7","executionInfo":{"status":"ok","timestamp":1688868579507,"user_tz":-120,"elapsed":2,"user":{"displayName":"mojtaba nayyeri","userId":"16541089370646536836"}},"outputId":"14a047dd-3dc4-4547-cdb2-cab77de1591f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x =  tensor([[1, 4],\n","        [2, 3]])\n","type = torch.int64\n","\n"," x =  tensor([[1., 4.],\n","        [2., 3.]])\n","type = torch.float32\n","\n"," y =  tensor([[1., 4.],\n","        [2., 3.]])\n","type = torch.float32\n","\n"," A random tensor:\n","tensor([[0.1560, 0.9761],\n","        [0.2683, 0.1891]])\n","\n"," Addition and Multiplication:\n","tensor([[2., 8.],\n","        [4., 6.]]) \n"," tensor([[ 1., 16.],\n","        [ 4.,  9.]]) \n"," tensor([[ 9., 16.],\n","        [ 8., 17.]])\n"]}]},{"cell_type":"markdown","source":["# Autograd:\n","PyTorch includes automated differentiation engine. To allow differantionation of a function f(w) w.r.t. the tensor w, the tensor should have \"requires_grad= True\", e.g., w = torch.tensor([[0.2, 0.4],[0.8, 0.1],[2.0, 0.5]], requires_grad= True). You can access the gradient by w.grad.\n","\n","##Task 4: Implementation of simple Feedforward Neural Network in Pytorch:\n","\n","In the \"Neural Networks Example\" task described in the PDF file, you manually performed the forward pass and gradient calculation. Now, in this new task, you will implement the same task but using tensors and PyTorch's autograd functionality. Once you compute the loss, you can use the loss.backward() function to perform backpropagation. Your goal is to compare the results of the forward pass, loss calculation, backpropagation, and parameter updates between these two approaches (manual gradient computation v.s. automated gradient computation)."],"metadata":{"id":"0AYA9tDXvw9Q"}},{"cell_type":"code","source":["# Define weights and biases\n","\n","\n","\n","# Implement Forward pass\n","\n","\n","\n","y = -1\n","\n","# Compute loss and take automated differentiation\n","\n","\n","# Print the results (forward pass, gradients, etc.) and compare with the results you obtained manually in \"Neural Network Example\" in PDF file.\n"],"metadata":{"id":"JnGon65XzgAx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Building a model:\n","\n","PyTorch provided various building blocks of Neural Neuworks such as linear model, various activation functions (relu, tanh, etc), convolution layer, etc.    \n","## Modules\n","\n","Modules are the second key component of Torch after tensors. Modules represent network layers, loss functions, and entire neural networks. Torch offers a large library of modules, which you can overview [here](https://pytorch.org/docs/stable/nn.html).\n","\n","A [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) can be easily implemented by inherting from `nn.Module` and implementing at least the `forward` method, which defines the computation your network makes during the forward pass. Due to Torch's autodifferentiation, the backward pass (including gradient computation) is performed automatically by Torch.\n","\n","The following cell implements an example Module for a simple linear model with quadratic features."],"metadata":{"id":"BNuokAKf-8Ia"}},{"cell_type":"code","source":["linear = nn.Linear(2,3) #w x + b\n","\n","print(\"Initial weights:\")\n","print(linear.weight)\n","print(\"\\nInitial biases:\")\n","print(linear.bias)\n","\n","linear.bias = nn.Parameter(torch.tensor([1.0,2.0,3.0]))\n","print(\"\\n Biases after setting:\")\n","print(linear.bias)\n","\n","h1 = linear(x.t())\n","print(h1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"coPwSClvAepU","executionInfo":{"status":"ok","timestamp":1688868587488,"user_tz":-120,"elapsed":343,"user":{"displayName":"mojtaba nayyeri","userId":"16541089370646536836"}},"outputId":"1c34afbe-14e5-4062-ca17-c61b3e98f75a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial weights:\n","Parameter containing:\n","tensor([[-0.6144,  0.1389],\n","        [ 0.3795,  0.2146],\n","        [-0.4978, -0.2421]], requires_grad=True)\n","\n","Initial biases:\n","Parameter containing:\n","tensor([ 0.0853, -0.5194, -0.7014], requires_grad=True)\n","\n"," Biases after setting:\n","Parameter containing:\n","tensor([1., 2., 3.], requires_grad=True)\n","tensor([[0.6634, 2.8087, 2.0180]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["##Task 5: Build a simple model in PyTorch:\n","\n","In task 4, you implemented a simple neural network (introduced in PDF file, the part \"neural network example\") by using tensor. In this task, you will implement the neural network of task 4 by using torch.nn.Linear and asnwer the same questions given in the task \"Neural Network example\" in pdf file, and compare the results."],"metadata":{"id":"Pip1ioffCTvC"}},{"cell_type":"code","source":["import torch.optim as optim\n","class SimpleNN(nn.Module):\n","\n","    def __init__(self):\n","        super(SimpleNN, self).__init__()\n","        # NN with one hidden layer (with 3 hidden nodes), and one output node\n","        # The input data is 2 dimensional\n","\n","    # define forward pass\n","    def forward(self, x):\n","\n","        return out\n","\n","x = torch.tensor([1.0, 2.0])\n","neural_net = SimpleNN()\n","yhat = neural_net.forward(x)\n","y = -1\n","\n","print('yhat = ', yhat)\n","\n","# define loss function\n","\n","loss =\n","\n","print('loss = ', loss)\n","\n","\n","optimizer = optim.SGD(neural_net.parameters(), lr=0.1)\n","optimizer.zero_grad()\n","\n","# Perform backward and NN parameters update by using backward and step functions\n","\n","# Print the results (forward pass, gradients, etc.) and compare with the results of task 4\n"],"metadata":{"id":"_23HLeJbDDVj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 6: Implementing a Feedforward Neural Network with PyTorch\n","\n","Implement the neural network in task 3 with PyTorch"],"metadata":{"id":"LpDQW_B4l7bB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LsFCp4CIDcj5"},"outputs":[],"source":["class FeedforwardNeuralNetworkRegressorPytorch(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        output_dim: int,\n","        hidden_dims: List[int],\n","        depth: int,\n","        learning_rate: float,\n","        epochs: int\n","    ):\n","        super(FeedforwardNeuralNetworkRegressorPytorch, self).__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.hidden_dims = hidden_dims\n","        self.depth = depth\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        # Create list for all weights and biases\n","        # By using nn.Linear and activation function and nn.sequential module, define the neural network\n","\n","        layer_sizes = [input_dim] + hidden_dims + [output_dim]\n","        layers = []\n","\n","\n","\n","\n","    def forward(self, x):\n","\n"]},{"cell_type":"code","source":["np.random.seed(2023)\n","\n","input_dim = X_train.shape[1]\n","output_dim = y_train.shape[1]\n","model = FeedforwardNeuralNetworkRegressorPytorch(input_dim, output_dim, [5], 1, 0.1, 100)\n","\n","# Define the loss function and optimizer\n","\n","# Training loop\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    # Forward pass\n","\n","\n","    # Backward pass and optimization\n","\n","\n","    # Print the loss for every 100 epochs\n","    if (epoch + 1) % 100 == 0:\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","y_pred = model(torch.from_numpy(X_test).to(torch.float32))\n","ytest = torch.from_numpy(y_test).to(torch.float32)\n","test_error = criterion(y_pred, ytest)\n","print(\"Test error:\", test_error)\n","\n","plt.figure()\n","plt.plot(train_error)\n","plt.show()"],"metadata":{"id":"j89fM8RMYrzY"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}